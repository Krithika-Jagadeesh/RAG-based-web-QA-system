# ğŸ“„ Gemini-Powered RAG Web QA System 

### ğŸ§­ Project Overview:
This project implements a Retrieval-Augmented Generation (RAG) pipeline that allows users to:

â€¢	Index content from public websites

â€¢	Ask natural language questions about that content

â€¢	Receive grounded, citation-rich answers generated by Gemini AI

The system is modular, open-source, and designed for extensibility. It uses:

â€¢	FastAPI for backend services

â€¢	Streamlit for user interface

â€¢	FAISS for vector-based retrieval

â€¢	SentenceTransformers for semantic embeddings

â€¢	Gemini AI (Google) for answer generation

### ğŸ§± System Architecture:
The pipeline follows a classic RAG structure:
1.	Document Ingestion
Web pages are scraped and cleaned to extract readable text.
2.	Chunking and Embedding
Text is split into manageable chunks and converted into vector embeddings using a pre-trained model.
3.	Indexing
Embeddings are stored in a FAISS index for fast similarity search.
4.	Retrieval
When a user submits a query, the system retrieves the most relevant chunks from the index.
5.	Prompt Construction
Retrieved chunks are formatted into a prompt with citations.
6.	Answer Generation
The prompt is sent to Gemini AI via the Google Generative AI SDK, which returns a grounded answer.
7.	User Interface
The answer is displayed in Streamlit, along with status codes and raw response data.

### ğŸ“‚ Project Structure according to the files:
app.py -- Streamlit frontend
main.py -- FastAPI backend with /index and /chat 
rag_generator.py -- Gemini AI generation logic 
rag_retriever.py -- FAISS-based retrieval 
utils_embedding.py -- Chunking + embedding 
utils_scraper.py -- Web scraping 
.env -- Gemini API key 

### ğŸ” API Integration:
The system uses Gemini AI via the Google Generative AI SDK. You must:
â€¢	Create a free API key from Google AI Studio
â€¢	Store the key in a .env file
â€¢	Load and configure the SDK using genai.configure(api_key=...)
The model used is;
â€˜models/gemini-pro-latestâ€™. This model supports text-only generation via generate_content().

### ğŸ§  Model Components:
â€¢	Embedding Model:
A lightweight transformer model (all-MiniLM-L6-v2) is used to convert text chunks and queries into semantic vectors.
â€¢	Vector Store:
FAISS is used for efficient similarity search over embedded chunks.
â€¢	Generation Model:
Gemini Pro generates answers based on retrieved context. Responses include citation markers like [1], [2], etc.

### ğŸ–¥ï¸ User Interface - Streamlit app:
â€¢	A section to index new URLs
â€¢	A section to ask questions
â€¢	Display of status codes and raw response text
â€¢	Markdown rendering of the final answer

### ğŸ”„ Workflow Summary:
ğŸ”¹ Indexing:
â€¢	User enters one or more URLs into the Streamlit â€œIndex New URLsâ€ box.
â€¢	The backend scrapes and processes each page.
â€¢	Text is chunked and embedded.
â€¢	Embeddings are stored in FAISS for retrieval.

ğŸ”¹ Question Answering:
â€¢	User enters a question in the â€œAsk a questionâ€¦â€ box.
â€¢	The query is embedded and compared against the indexed chunks.
â€¢	Top-k relevant chunks are selected.
â€¢	These chunks are injected into a prompt with citation markers.
â€¢	The prompt is sent to Gemini AI via SDK.
â€¢	The generated answer is returned and displayed.

### ğŸ§ª Sample Inputs:
URLs to Index:
â€¢	Huyen Chipâ€™s GenAI platform post
https://huyenchip.com/2024/07/25/genai-platform.html
â€¢	Jina AIâ€™s ColBERT search explanation
https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/
â€¢	Lilian Wengâ€™s hallucination article
https://lilianweng.github.io/posts/2024-07-07-hallucinatio
â€¢	Quora Engineeringâ€™s embedding search write-up
https://quoraengineering.quora.com/Building-Embedding-Search-at-Quora

Questions to Ask:
â€¢	â€œWhat is Build Time in GenAI platforms?â€
â€¢	â€œHow does ColBERT differ from dense retrieval?â€
â€¢	â€œWhat causes hallucinations in LLMs?â€
â€¢	â€œHow does Quora use embeddings for multilingual search?â€

### ğŸ“ˆ Output Format:
Answers are returned in natural language, grounded in retrieved context, and include citation markers. 
Example: â€œBuild Time refers to the latency between prompt submission and model readiness. It is distinct from Time to First Token (TFT) and Total Latency [1].â€

### ğŸ› ï¸ Error Handling:
â€¢	If no documents are indexed, the system returns a clear error.
â€¢	Gemini API errors (e.g., invalid model name or quota limits) are caught and displayed.
â€¢	If the answer cannot be generated, the system returns a descriptive fallback message.

### ğŸ”„ Extensibility:
This system is designed to be modular and extensible. Future enhancements could include:
â€¢	PDF ingestion
â€¢	Citation highlighting
â€¢	Chunk preview in UI
â€¢	Reranking with ColBERT or BGE
â€¢	Multi-turn chat memory
â€¢	Switchable model dropdown (Gemini, Groq, Hugging Face)


